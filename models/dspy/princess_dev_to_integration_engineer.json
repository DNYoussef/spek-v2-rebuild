{
  "delegate.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"authentication-service\", \"user-service\", \"api-gateway\"], \"requirements\": \"Integrate new authentication flow with user service\"}",
        "reasoning": "For this authentication integration task, I'll assign the integration-engineer drone since it specializes in connecting system components and handling security-critical workflows. The task requires careful coordination between authentication flows, user management, and API gateway configuration. The integration points need to cover the full authentication lifecycle including token management, user session handling, and secure communication between services.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate authentication service with user service through API gateway\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Auth service → User service (identity verification)\",\n            \"API gateway → Auth service (token validation)\",\n            \"User service → Auth service (session management)\",\n            \"Auth service → API gateway (security policies)\"\n        ],\n        \"data_flow\": \"Login request → Auth validates → User lookup → Generate token → Configure gateway\",\n        \"contracts\": [\n            \"AuthenticationAPI v1\",\n            \"UserServiceAPI v2\",\n            \"Gateway security policies\"\n        ],\n        \"error_handling\": \"Invalid credentials, token expiration, service unavailability, rate limiting\"\n    },\n    \"acceptance_criteria\": [\n        \"Authentication flow successfully validates users\",\n        \"Tokens properly generated and validated\",\n        \"User sessions managed correctly\",\n        \"API gateway enforces security policies\",\n        \"Error scenarios handled gracefully\",\n        \"Performance meets latency requirements (<200ms)\",\n        \"All security best practices implemented\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"payment-gateway\", \"order-service\", \"notification-service\"], \"requirements\": \"Connect payment processing with order fulfillment\"}",
        "reasoning": "For this integration task, I'll assign the integration-engineer drone since it specializes in connecting different services and handling complex data flows. The task requires careful coordination between payment processing, order management, and notification systems. Key considerations include:\n- Transaction consistency across services\n- Payment state management\n- Asynchronous notification handling\n- Error recovery and rollback scenarios\n- Idempotency in payment processing",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate payment gateway with order fulfillment system and notification service\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Order service → Payment gateway (process payment)\",\n            \"Payment gateway → Order service (payment status)\",\n            \"Order service → Notification service (order updates)\",\n            \"Payment gateway → Notification service (payment events)\"\n        ],\n        \"data_flow\": \"Create order → Process payment → Update order → Send notifications\",\n        \"contracts\": [\n            \"Payment gateway API\",\n            \"Order status transitions\",\n            \"Notification templates\"\n        ],\n        \"error_handling\": [\n            \"Payment failures\",\n            \"Gateway timeouts\",\n            \"Notification delivery\",\n            \"Duplicate payments\"\n        ]\n    },\n    \"acceptance_criteria\": [\n        \"Payment processing successfully updates order status\",\n        \"Failed payments trigger appropriate order state changes\",\n        \"Notifications sent for all relevant payment and order events\",\n        \"Transaction rollback works in failure scenarios\",\n        \"Idempotency maintained across retries\",\n        \"Payment reconciliation process verified\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"frontend-app\", \"graphql-api\", \"rest-services\"], \"requirements\": \"Unify data fetching through GraphQL layer\"}",
        "reasoning": "For this integration task, I'll assign the integration-engineer drone since it specializes in connecting different system components and implementing data flow patterns. The task requires careful coordination between frontend, GraphQL, and REST services, with particular attention to schema design, resolver implementation, and error handling. The integration needs to ensure consistent data access patterns while maintaining performance.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Implement GraphQL integration layer between frontend and REST services\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Frontend \\u2192 GraphQL (queries/mutations)\",\n            \"GraphQL \\u2192 REST services (resolvers)\",\n            \"Schema \\u2192 Type system (validation)\",\n            \"GraphQL \\u2192 Caching layer (performance)\"\n        ],\n        \"data_flow\": \"Frontend query \\u2192 GraphQL validates \\u2192 Resolver calls REST \\u2192 Transform response \\u2192 Return data\",\n        \"contracts\": [\n            \"GraphQL schema\",\n            \"REST endpoints\",\n            \"Type definitions\"\n        ],\n        \"error_handling\": \"REST failures, schema validation, partial responses, timeout handling\"\n    },\n    \"acceptance_criteria\": [\n        \"GraphQL schema correctly reflects REST data models\",\n        \"All frontend queries successfully resolve\",\n        \"Error handling properly propagates REST errors\",\n        \"Response caching improves performance\",\n        \"Schema documentation is complete\",\n        \"Integration tests pass\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"mobile-app\", \"sync-service\", \"cloud-storage\"], \"requirements\": \"Implement offline-first sync between mobile and cloud\"}",
        "reasoning": "For this offline-first sync integration task, I'll assign the integration-engineer drone. The task requires careful handling of conflict resolution, queue management for pending changes, and robust error handling for intermittent connectivity. The integration needs to ensure data consistency while providing seamless user experience during offline periods.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Implement offline-first data synchronization between mobile app and cloud storage\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Mobile app \\u2192 local storage (offline data)\",\n            \"Sync service \\u2192 conflict resolution\",\n            \"Cloud storage \\u2192 master data store\",\n            \"Background sync \\u2192 queue management\"\n        ],\n        \"data_flow\": \"Local changes \\u2192 Queue updates \\u2192 Sync when online \\u2192 Resolve conflicts \\u2192 Update local\",\n        \"contracts\": [\n            \"Sync protocol v1\",\n            \"Conflict resolution rules\",\n            \"Queue schema\"\n        ],\n        \"error_handling\": \"Network failures, conflict resolution, queue overflow, version mismatches\"\n    },\n    \"acceptance_criteria\": [\n        \"Data persists offline\",\n        \"Changes sync when online\",\n        \"Conflicts resolve automatically\",\n        \"Background sync works\",\n        \"Queue manages pending changes\",\n        \"Network transitions handled\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"analytics-service\", \"event-bus\", \"data-warehouse\"], \"requirements\": \"Stream analytics events to data warehouse\"}",
        "reasoning": "For this integration task, I'll assign the integration-engineer drone since it involves connecting multiple data systems and setting up event streaming pipelines. The task requires expertise in event-driven architectures, data warehousing, and ensuring reliable data flow. The integration needs to handle high throughput, maintain data consistency, and implement proper error handling for failed events.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate analytics event streaming pipeline to data warehouse\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Analytics service → Event bus (publish events)\",\n            \"Event bus → Stream processor (transform)\",\n            \"Stream processor → Data warehouse (load)\",\n            \"Monitoring → Pipeline health (alerts)\"\n        ],\n        \"data_flow\": \"Collect analytics → Stream events → Transform → Load warehouse → Query data\",\n        \"contracts\": [\n            \"Event schema\",\n            \"Warehouse schema\",\n            \"Stream processing rules\"\n        ],\n        \"error_handling\": \"Event validation, retry mechanism, dead letter queue, data loss prevention\"\n    },\n    \"acceptance_criteria\": [\n        \"Events flow end-to-end without loss\",\n        \"Data warehouse updates in near real-time\",\n        \"Failed events properly handled and logged\",\n        \"Monitoring alerts configured and tested\",\n        \"Pipeline handles peak load conditions\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"search-service\", \"elasticsearch\", \"product-catalog\"], \"requirements\": \"Keep search index in sync with product catalog\"}",
        "reasoning": "For this integration task, I'll assign the integration-engineer drone since it specializes in connecting different systems and ensuring data consistency. The task requires setting up real-time synchronization between the product catalog and Elasticsearch, including handling index updates, deletions, and error scenarios. The integration needs to maintain data consistency while being performant.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate product catalog changes with Elasticsearch search index\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Product catalog events → message queue\",\n            \"Message queue → search indexer\",\n            \"Search indexer → Elasticsearch\",\n            \"Bulk reindex process → data consistency\"\n        ],\n        \"data_flow\": \"Product update → Event published → Indexer processes → Search index updated\",\n        \"contracts\": [\n            \"Product event schema\",\n            \"Search document mapping\"\n        ],\n        \"error_handling\": \"Failed indexing, queue backlog, version conflicts, partial updates\"\n    },\n    \"acceptance_criteria\": [\n        \"Product updates reflect in search within 5 seconds\",\n        \"Bulk reindex process works without downtime\",\n        \"Failed events retry automatically\",\n        \"Index versioning handles concurrent updates\",\n        \"Search results match product catalog exactly\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"ci-pipeline\", \"github-actions\", \"deployment-service\"], \"requirements\": \"Automate deployment pipeline from CI\"}",
        "reasoning": "For this CI/CD integration task, I'll assign the integration-engineer drone since it specializes in connecting different systems and establishing automated workflows. The task requires setting up GitHub Actions workflows that connect with our deployment service, requiring careful handling of secrets, environment configurations, and deployment strategies. The integration needs to ensure reliable, secure automated deployments with proper error handling and rollback capabilities.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate GitHub Actions CI pipeline with deployment service\",\n    \"payload\": {\n        \"integration_points\": [\n            \"GitHub Actions \\u2192 Build/Test (CI workflow)\",\n            \"GitHub Actions \\u2192 Deployment service (CD trigger)\",\n            \"Deployment service \\u2192 Environment (deployment execution)\",\n            \"Status updates \\u2192 GitHub (deployment feedback)\"\n        ],\n        \"data_flow\": \"PR merge \\u2192 CI build \\u2192 Tests pass \\u2192 Trigger deploy \\u2192 Environment update\",\n        \"contracts\": [\n            \"GitHub Actions workflow syntax\",\n            \"Deployment API contract\"\n        ],\n        \"error_handling\": \"Build failures, deployment timeouts, rollback procedures\"\n    },\n    \"acceptance_criteria\": [\n        \"CI pipeline automatically triggers on PR merge\",\n        \"Deployment service receives and processes deployment requests\",\n        \"Deployment status correctly reflected in GitHub\",\n        \"Failed deployments trigger automatic rollback\",\n        \"Secrets and credentials properly secured\",\n        \"Pipeline metrics and logs accessible\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"monitoring-service\", \"prometheus\", \"grafana\", \"alertmanager\"], \"requirements\": \"Set up comprehensive monitoring and alerting\"}",
        "reasoning": "For this monitoring integration task, I'll assign the integration-engineer drone since it involves connecting multiple monitoring components (Prometheus, Grafana, Alertmanager) into a cohesive observability stack. The task requires careful configuration of metrics collection, dashboard setup, and alert routing. The integration points need to ensure proper data flow between components while maintaining reliability and performance.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate Prometheus, Grafana, and Alertmanager for comprehensive monitoring\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Services → Prometheus (metrics collection)\",\n            \"Prometheus → Grafana (data source)\",\n            \"Prometheus → Alertmanager (alert routing)\",\n            \"Alertmanager → notification channels\"\n        ],\n        \"data_flow\": \"Service metrics → Prometheus scrape → Grafana visualize → Alertmanager notify\",\n        \"configurations\": {\n            \"prometheus\": [\"scrape configs\", \"recording rules\", \"alert rules\"],\n            \"grafana\": [\"datasources\", \"dashboards\", \"plugins\"],\n            \"alertmanager\": [\"routing tree\", \"notification templates\", \"silence rules\"]\n        },\n        \"error_handling\": \"Metric collection failures, alert delivery, dashboard rendering\"\n    },\n    \"acceptance_criteria\": [\n        \"Prometheus successfully scraping all service metrics\",\n        \"Grafana dashboards showing real-time metrics\",\n        \"Alert rules properly configured and firing\",\n        \"Notifications reaching correct channels\",\n        \"Recording rules optimizing query performance\",\n        \"High availability setup for all components\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"email-service\", \"template-engine\", \"smtp-provider\"], \"requirements\": \"Integrate email sending with templating system\"}",
        "reasoning": "For this email service integration task, I'll assign the integration-engineer drone since it involves connecting multiple components (email service, template engine, SMTP) with proper error handling and monitoring. The task requires careful handling of template rendering, SMTP connection management, and ensuring reliable email delivery. The integration needs to support both transactional and batch email sending scenarios.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate email service with template engine and SMTP provider\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Email service → Template engine (render emails)\",\n            \"Template engine → SMTP provider (send emails)\",\n            \"Email service → Queue system (batch sending)\",\n            \"Monitoring → SMTP metrics (delivery tracking)\"\n        ],\n        \"data_flow\": \"Request → Load template → Render content → Queue → SMTP send\",\n        \"contracts\": [\n            \"Template API contract\",\n            \"SMTP configuration\",\n            \"Email queue schema\"\n        ],\n        \"error_handling\": [\n            \"Template rendering failures\",\n            \"SMTP connection issues\",\n            \"Queue processing errors\",\n            \"Retry mechanisms\"\n        ]\n    },\n    \"acceptance_criteria\": [\n        \"Templates render correctly with variables\",\n        \"Emails send successfully through SMTP\",\n        \"Queue handles batch processing\",\n        \"Failed sends are retried appropriately\",\n        \"Delivery metrics are tracked\",\n        \"HTML and plain text formats supported\"\n    ]\n}"
      },
      {
        "augmented": true,
        "phase": "integration",
        "context": "{\"previous_phase\": \"code\", \"components\": [\"file-upload-service\", \"s3\", \"image-processing-service\"], \"requirements\": \"Process uploaded images asynchronously\"}",
        "reasoning": "For this integration task, I'll assign the integration-engineer drone since we need to connect multiple services (file upload, S3 storage, and image processing) in an asynchronous workflow. The task requires setting up proper event handling, ensuring reliable file transfers, and implementing robust error handling. The integration needs to maintain data consistency across services while handling large files efficiently.",
        "drone_task": "{\n    \"drone_id\": \"integration-engineer\",\n    \"task_type\": \"integrate\",\n    \"description\": \"Integrate asynchronous image processing pipeline with upload service and S3\",\n    \"payload\": {\n        \"integration_points\": [\n            \"Upload service → S3 (initial storage)\",\n            \"S3 event → processing service (trigger)\",\n            \"Processing service → S3 (processed images)\",\n            \"Processing service → upload service (status updates)\"\n        ],\n        \"data_flow\": \"Upload → S3 raw → Process async → Store processed → Notify complete\",\n        \"contracts\": [\n            \"S3 event format\",\n            \"Processing service API\",\n            \"Status update protocol\"\n        ],\n        \"error_handling\": [\n            \"Upload failures\",\n            \"Processing timeouts\",\n            \"S3 throttling\",\n            \"Partial processing results\"\n        ]\n    },\n    \"acceptance_criteria\": [\n        \"Files upload successfully to S3\",\n        \"Processing triggers automatically on upload\",\n        \"Processed images stored correctly\",\n        \"Status updates propagate to upload service\",\n        \"Failed processing handled gracefully\",\n        \"Large files (>100MB) processed without timeout\"\n    ]\n}"
      }
    ],
    "signature": {
      "instructions": "Delegate task to drone agent with clear instructions.\n\nYou are a Princess agent coordinating your specialized drone hive.\nYour role is to take high-level workflow phases and convert them\ninto specific, actionable tasks for your drone agents.\n\nPrincess-Dev drones: coder, reviewer, debugger, integration-engineer\nPrincess-Quality drones: tester, nasa-enforcer, theater-detector, fsm-analyzer\nPrincess-Coordination drones: orchestrator, planner, cost-tracker\n\nEach drone task must have:\n- Specific drone agent assignment\n- Task type matching drone capabilities\n- Clear context from previous phases\n- Concrete deliverables\n- Quality gates/acceptance criteria\n\nFollow the 26 prompt engineering principles:\n- Clarity: Unambiguous instructions\n- Context: Include relevant phase results\n- Constraints: Respect drone capabilities\n- Structure: Output valid JSON format",
      "fields": [
        {
          "prefix": "Phase:",
          "description": "Development phase to execute (design, code, test, review, etc.)"
        },
        {
          "prefix": "Context:",
          "description": "Context and results from previous phases that this phase depends on"
        },
        {
          "prefix": "Reasoning: Let's think step by step in order to",
          "description": "Delegation reasoning explaining drone selection and task structure"
        },
        {
          "prefix": "Drone Task:",
          "description": "Structured task for drone agent as JSON object. Must have: {'drone_id': str, 'task_type': str, 'description': str, 'payload': dict, 'acceptance_criteria': list[str]}"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.3",
      "cloudpickle": "3.0"
    }
  }
}
