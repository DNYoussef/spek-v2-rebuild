import queue
from queue import Queue, Empty
from typing import Any, Dict, List, Optional, Union, Tuple, Callable, Set

"""
Stream Processing Engine for Incremental Analysis
================================================

Event-driven architecture for real-time connascence analysis with incremental
processing capabilities. Enables continuous integration workflows with sub-second
feedback for code changes.

Features:
- Event-driven file change detection with debouncing
- Incremental AST diff processing for efficient analysis
- Partial analysis result caching and intelligent merging
- Stream-based violation detection with dependency tracking
- Real-time results streaming for CI/CD integration
- Backpressure handling for large-scale projects
"""

from collections import defaultdict, deque
from pathlib import Path
from typing import Any, AsyncGenerator, Callable, Dict, List, Optional, Set, Union
import hashlib
import json
import logging
import time

from dataclasses import dataclass, field
import asyncio
import threading
import weakref

try:
    from watchdog.events import FileSystemEventHandler, FileSystemEvent
except ImportError:
    # Fallback for when watchdog is not available
    class FileSystemEventHandler:
        """Fallback file system event handler."""

    class FileSystemEvent:
        """Fallback file system event."""
    def __init__(self, src_path=''):
            self.src_path = src_path
            self.is_directory = False

logger = logging.getLogger(__name__)

@dataclass
class FileChange:
    """Represents a file change event."""
    file_path: Path
    change_type: str  # 'created', 'modified', 'deleted', 'moved'
    timestamp: float
    content_hash: Optional[str] = None
    previous_hash: Optional[str] = None
    size_bytes: int = 0
    
    def __post_init__(self):
        """Validate change data."""
        assert self.change_type in ['created', 'modified', 'deleted', 'moved'], \
            f"Invalid change_type: {self.change_type}"
        assert self.timestamp > 0, "timestamp must be positive"

@dataclass
class AnalysisRequest:
    """Represents an analysis request for processing."""
    request_id: str
    file_changes: List[FileChange]
    priority: int = 0  # Higher priority = process first
    requested_at: float = field(default_factory=time.time)
    dependencies: Set[str] = field(default_factory=set)  # File paths this analysis depends on
    analysis_type: str = "incremental"  # 'incremental', 'full', 'targeted'
    
    def __post_init__(self):
        """Validate request data."""
        assert self.request_id, "request_id cannot be empty"
        assert 0 <= self.priority <= 10, "priority must be 0-10"

@dataclass
class AnalysisResult:
    """Represents analysis result from stream processing."""
    request_id: str
    file_path: str
    violations: List[Dict[str, Any]]
    processing_time_ms: int
    analysis_type: str
    timestamp: float
    cache_hit: bool = False
    dependencies_analyzed: Set[str] = field(default_factory=set)
    metadata: Dict[str, Any] = field(default_factory=dict)

class FileWatcher(FileSystemEventHandler):
    """
    File system watcher for detecting Python file changes.
    
    Implements debouncing and filtering to reduce noise in file change events.
    """
    
    def __init__(self, 
                callback: Callable[[List[FileChange]], None],
                debounce_seconds: float = 0.5,
                file_patterns: Optional[List[str]] = None):
        """
        Initialize file watcher.
        
        Args:
            callback: Callback function for file changes
            debounce_seconds: Debounce delay to batch rapid changes
            file_patterns: File patterns to watch (default: Python files)
        """
        super().__init__()
        self.callback = callback
        self.debounce_seconds = debounce_seconds
        self.file_patterns = file_patterns or ["*.py"]
        
        # Change tracking and debouncing
        self._pending_changes: Dict[str, FileChange] = {}
        self._debounce_timers: Dict[str, threading.Timer] = {}
        self._lock = threading.RLock()
        
        # File content hashing for change detection
        self._file_hashes: Dict[str, str] = {}
    
    def on_modified(self, event: FileSystemEvent) -> None:
        """Handle file modification events."""
        if not event.is_directory and self._should_process_file(event.src_path):
            self._handle_change(event.src_path, 'modified')
    
    def on_created(self, event: FileSystemEvent) -> None:
        """Handle file creation events."""
        if not event.is_directory and self._should_process_file(event.src_path):
            self._handle_change(event.src_path, 'created')
    
    def on_deleted(self, event: FileSystemEvent) -> None:
        """Handle file deletion events."""
        if not event.is_directory and self._should_process_file(event.src_path):
            self._handle_change(event.src_path, 'deleted')
    
    def on_moved(self, event: FileSystemEvent) -> None:
        """Handle file move/rename events."""
        if (not event.is_directory and 
            hasattr(event, 'dest_path') and 
            self._should_process_file(event.dest_path)):
            self._handle_change(event.dest_path, 'moved')
    
    def _should_process_file(self, file_path: str) -> bool:
        """Check if file should be processed based on patterns."""
        path_obj = Path(file_path)
        
        # Check file patterns
        for pattern in self.file_patterns:
            if path_obj.match(pattern):
                return True
                
        return False
    
    def _handle_change(self, file_path: str, change_type: str) -> None:
        """Handle file change with debouncing."""
        with self._lock:
            file_path_str = str(file_path)
            
            # Cancel existing timer for this file
            if file_path_str in self._debounce_timers:
                self._debounce_timers[file_path_str].cancel()
            
            # Calculate content hash if file exists and is readable
            content_hash = None
            previous_hash = self._file_hashes.get(file_path_str)
            size_bytes = 0
            
            if change_type != 'deleted':
                try:
                    path_obj = Path(file_path)
                    if path_obj.exists():
                        size_bytes = path_obj.stat().st_size
                        # Only hash small files to avoid performance issues
                        if size_bytes < 1024 * 1024:  # 1MB limit
                            with open(path_obj, 'rb') as f:
                                content = f.read()
                                content_hash = hashlib.sha256(content).hexdigest()[:16]
                            self._file_hashes[file_path_str] = content_hash
                except Exception as e:
                    logger.debug(f"Failed to hash {file_path}: {e}")
            else:
                # File deleted - remove from hash tracking
                self._file_hashes.pop(file_path_str, None)
            
            # Skip if content hasn't actually changed
            if (change_type == 'modified' and 
                content_hash and previous_hash and 
                content_hash == previous_hash):
                return
            
            # Create change record
            change = FileChange(
                file_path=Path(file_path),
                change_type=change_type,
                timestamp=time.time(),
                content_hash=content_hash,
                previous_hash=previous_hash,
                size_bytes=size_bytes
            )
            
            self._pending_changes[file_path_str] = change
            
            # Set debounce timer
            timer = threading.Timer(self.debounce_seconds, self._flush_changes)
            self._debounce_timers[file_path_str] = timer
            timer.start()
    
    def _flush_changes(self) -> None:
        """Flush pending changes to callback."""
        with self._lock:
            if self._pending_changes:
                changes = list(self._pending_changes.values())
                self._pending_changes.clear()
                self._debounce_timers.clear()
                
                # Execute callback with accumulated changes
                try:
                    self.callback(changes)
                except Exception as e:
                    logger.error(f"File change callback failed: {e}")

class StreamProcessor:
    """
    Core stream processing engine for incremental analysis.

    Handles event-driven analysis with intelligent caching, dependency tracking,
    and real-time result streaming.
    """

    def __init__(self,
                analyzer_factory: Optional[Callable[[], Any]] = None,
                max_queue_size: int = 1000,
                max_workers: int = 4,
                cache_size: int = 10000,
                buffer_size: int = 1000,
                flush_interval: float = 5.0):
        """
        Initialize stream processor.
        
        Args:
            analyzer_factory: Factory function to create analyzer instances
            max_queue_size: Maximum analysis request queue size (NASA Rule 7)
            max_workers: Maximum concurrent worker threads
            cache_size: Maximum cache entries to maintain
        """
        assert 10 <= max_queue_size <= 50000, "max_queue_size must be 10-50000"
        assert 1 <= max_workers <= 16, "max_workers must be 1-16"
        assert 100 <= cache_size <= 100000, "cache_size must be 100-100000"
        
        self.analyzer_factory = analyzer_factory or self._default_analyzer_factory
        self.max_queue_size = max_queue_size
        self.max_workers = max_workers
        self.buffer_size = buffer_size
        self.flush_interval = flush_interval

        # Component integrations
        self._cache = None
        self._aggregator = None
        
        # Request processing
        self._request_queue: deque = deque(maxlen=max_queue_size)
        self._processing_queue = asyncio.Queue(maxsize=max_queue_size)
        self._results_queue = asyncio.Queue(maxsize=max_queue_size * 2)
        
        # Worker management
        self._workers: List[asyncio.Task] = []
        self._running = False
        self._worker_semaphore = asyncio.Semaphore(max_workers)
        
        # Caching and optimization
        self._result_cache: Dict[str, AnalysisResult] = {}
        self._cache_access_times: Dict[str, float] = {}
        self.cache_size = cache_size
        
        # Dependency tracking
        self._file_dependencies: Dict[str, Set[str]] = defaultdict(set)
        self._reverse_dependencies: Dict[str, Set[str]] = defaultdict(set)
        
        # File watching
        self.file_watcher: Optional[FileWatcher] = None
        self.observer: Optional[Observer] = None
        self._watched_directories: Set[str] = set()
        
        # Statistics
        self._stats = {
            "requests_processed": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "processing_time_ms": 0,
            "queue_overflows": 0,
            "dependency_invalidations": 0
        }
        
        # Result callbacks
        self._result_callbacks: List[Callable[[AnalysisResult], None]] = []
        self._batch_callbacks: List[Callable[[List[AnalysisResult]], None]] = []

    def _default_analyzer_factory(self):
        """Default analyzer factory if none provided."""
        try:
            from ..unified_analyzer import UnifiedConnascenceAnalyzer
            return UnifiedConnascenceAnalyzer()
        except ImportError:
            return None

    def set_cache(self, cache):
        """Set incremental cache for the processor."""
        self._cache = cache

    def set_aggregator(self, aggregator):
        """Set result aggregator for the processor."""
        self._aggregator = aggregator

    def process_content(self, content: str) -> Dict[str, Any]:
        """Process file content and return analysis results."""
        try:
            import ast
            import sys
            from pathlib import Path

            # Add parent directory to path for imports
            analyzer_path = Path(__file__).parent.parent
            if str(analyzer_path) not in sys.path:
                sys.path.insert(0, str(analyzer_path))

            from detectors import (
                PositionDetector, MagicLiteralDetector, AlgorithmDetector,
                GodObjectDetector, TimingDetector, ConventionDetector,
                ValuesDetector, ExecutionDetector
            )

            # Parse AST
            tree = ast.parse(content)
            source_lines = content.splitlines()

            # Run detectors
            all_violations = []
            detectors = [
                PositionDetector("stream", source_lines),
                MagicLiteralDetector("stream", source_lines),
                AlgorithmDetector("stream", source_lines),
                GodObjectDetector("stream", source_lines),
                TimingDetector("stream", source_lines),
                ConventionDetector("stream", source_lines),
                ValuesDetector("stream", source_lines),
                ExecutionDetector("stream", source_lines)
            ]

            for detector in detectors:
                try:
                    violations = detector.detect_violations(tree)
                    all_violations.extend(violations)
                except Exception as e:
                    logger.error(f"Detector {detector.__class__.__name__} failed: {e}")

            return {
                "violations": [self._violation_to_dict(v) for v in all_violations],
                "lines_analyzed": len(source_lines),
                "detectors_run": len(detectors)
            }

        except Exception as e:
            logger.error(f"Content processing failed: {e}")
            return {"violations": [], "error": str(e)}

    def process_file_stream(self, file_path: str, content: str) -> Dict[str, Any]:
        """Process file stream for analysis with real implementation."""
        try:
            import ast
            violations = []

            # Parse the content to detect real violations
            try:
                tree = ast.parse(content)
                source_lines = content.splitlines()

                # Real AST analysis for violations
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        # Check for too many parameters (Position connascence)
                        if len(node.args.args) > 5:
                            violations.append({
                                "type": "position",
                                "line": node.lineno,
                                "description": f"Function {node.name} has too many parameters ({len(node.args.args)})",
                                "severity": "medium",
                                "file_path": file_path
                            })

                        # Check for long functions (complexity connascence)
                        func_lines = getattr(node, 'end_lineno', node.lineno) - node.lineno + 1
                        if func_lines > 60:  # NASA Rule 4
                            violations.append({
                                "type": "execution",
                                "line": node.lineno,
                                "description": f"Function {node.name} is too long ({func_lines} lines)",
                                "severity": "high",
                                "file_path": file_path
                            })

                    elif isinstance(node, ast.ClassDef):
                        # Check for god objects
                        class_methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]
                        if len(class_methods) > 20:
                            violations.append({
                                "type": "identity",
                                "line": node.lineno,
                                "description": f"Class {node.name} is a god object ({len(class_methods)} methods)",
                                "severity": "critical",
                                "file_path": file_path
                            })

                # Check for magic literals
                for node in ast.walk(tree):
                    if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
                        if node.value not in [0, 1, -1] and not isinstance(node.value, bool):
                            violations.append({
                                "type": "value",
                                "line": node.lineno,
                                "description": f"Magic literal detected: {node.value}",
                                "severity": "low",
                                "file_path": file_path
                            })

            except SyntaxError as e:
                violations.append({
                    "type": "syntax",
                    "line": e.lineno or 1,
                    "description": f"Syntax error: {e.msg}",
                    "severity": "critical",
                    "file_path": file_path
                })

            result = {
                "violations": violations,
                "file_path": file_path,
                "lines_analyzed": len(content.splitlines()),
                "violations_found": len(violations)
            }

            # Update aggregator if available
            if self._aggregator:
                self._aggregator.update(result)

            return result

        except Exception as e:
            logger.error(f"File stream processing failed for {file_path}: {e}")
            return {"error": str(e), "file_path": file_path, "violations": []}

    def process_file_change(self, file_path: str, changes: Dict[str, Any]):
        """Process file change event for streaming analysis."""
        try:
            if not path_exists(file_path):
                return

            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            return self.process_file_stream(file_path, content)

        except Exception as e:
            logger.error(f"File change processing failed for {file_path}: {e}")
            return {"error": str(e)}
    
    async def start(self) -> None:
        """Start the stream processor."""
        if self._running:
            logger.warning("Stream processor already running")
            return
            
        self._running = True
        
        # Start worker tasks
        for i in range(self.max_workers):
            worker = asyncio.create_task(
                self._worker_loop(),
                name=f"StreamWorker-{i}"
            )
            self._workers.append(worker)
            
        logger.info(f"Stream processor started with {self.max_workers} workers")
    
    async def stop(self) -> None:
        """Stop the stream processor."""
        if not self._running:
            return
            
        self._running = False
        
        # Stop file watching
        if self.observer:
            self.observer.stop()
            self.observer.join()
            
        # Cancel all worker tasks
        for worker in self._workers:
            worker.cancel()
        
        # Wait for workers to finish
        if self._workers:
            await asyncio.gather(*self._workers, return_exceptions=True)
            
        self._workers.clear()
        logger.info("Stream processor stopped")
    
    def start_watching(self, directories: List[Union[str, Path]]) -> None:
        """Start watching directories for file changes."""
        if not WATCHDOG_AVAILABLE:
            logger.warning("File watching not available - install watchdog package")
            return
            
        if self.observer:
            logger.warning("File watching already active")
            return
            
        # Create file watcher with callback
        self.file_watcher = FileWatcher(
            callback=self._handle_file_changes,
            debounce_seconds=0.5,
            file_patterns=["*.py"]
        )
        
        # Create observer and watch directories
        self.observer = Observer()
        
        for directory in directories:
            dir_path = Path(directory)
            if dir_path.exists() and dir_path.is_dir():
                self.observer.schedule(
                    self.file_watcher, 
                    str(dir_path), 
                    recursive=True
                )
                self._watched_directories.add(str(dir_path))
                logger.info(f"Watching directory: {dir_path}")
        
        self.observer.start()
        logger.info(f"File watching started for {len(self._watched_directories)} directories")
    
    def stop_watching(self) -> None:
        """Stop watching directories."""
        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None
            self.file_watcher = None
            self._watched_directories.clear()
            logger.info("File watching stopped")
    
    def _handle_file_changes(self, changes: List[FileChange]) -> None:
        """Handle file changes from file watcher."""
        if not changes:
            return
            
        # Create analysis request from file changes
        request = AnalysisRequest(
            request_id=f"watch_{int(time.time() * 1000)}",
            file_changes=changes,
            priority=5,  # Medium priority for file changes
            analysis_type="incremental"
        )
        
        # Add to processing queue
        asyncio.create_task(self._enqueue_request(request))
    
    async def submit_request(self, request: AnalysisRequest) -> str:
        """
        Submit analysis request for processing.
        
        Args:
            request: Analysis request to process
            
        Returns:
            Request ID for tracking
        """
        await self._enqueue_request(request)
        return request.request_id
    
    async def _enqueue_request(self, request: AnalysisRequest) -> None:
        """Enqueue analysis request with backpressure handling."""
        try:
            await self._processing_queue.put(request)
            logger.debug(f"Enqueued analysis request: {request.request_id}")
        except asyncio.QueueFull:
            self._stats["queue_overflows"] += 1
            logger.warning(f"Request queue full - dropping request: {request.request_id}")
    
    async def _worker_loop(self) -> None:
        """Main worker loop for processing analysis requests."""
        logger.debug(f"Worker started: {asyncio.current_task().get_name()}")
        
        while self._running:
            try:
                # Wait for request with timeout
                request = await asyncio.wait_for(
                    self._processing_queue.get(), 
                    timeout=1.0
                )
                
                async with self._worker_semaphore:
                    await self._process_request(request)
                    
            except asyncio.TimeoutError:
                continue  # Normal timeout, check if still running
            except Exception as e:
                logger.error(f"Worker error: {e}")
                await asyncio.sleep(1.0)  # Back off on errors
        
        logger.debug(f"Worker stopped: {asyncio.current_task().get_name()}")
    
    async def _process_request(self, request: AnalysisRequest) -> None:
        """
        Process individual analysis request.
        
        NASA Rule 4: Function under 60 lines
        """
        start_time = time.time()
        
        try:
            # Check cache for existing results
            cached_results = self._check_cache(request)
            if cached_results:
                await self._emit_results(cached_results)
                return
            
            # Process file changes
            results = []
            for file_change in request.file_changes:
                if file_change.change_type == 'deleted':
                    # Handle file deletion
                    result = self._handle_file_deletion(file_change, request)
                else:
                    # Analyze file
                    result = await self._analyze_file_change(file_change, request)
                    
                if result:
                    results.append(result)
            
            # Cache results
            self._cache_results(request, results)
            
            # Emit results
            await self._emit_results(results)
            
            # Update statistics
            processing_time_ms = int((time.time() - start_time) * 1000)
            self._stats["requests_processed"] += 1
            self._stats["processing_time_ms"] += processing_time_ms
            
            logger.debug(f"Processed request {request.request_id} in {processing_time_ms}ms")
            
        except Exception as e:
            logger.error(f"Failed to process request {request.request_id}: {e}")
    
    def _check_cache(self, request: AnalysisRequest) -> List[AnalysisResult]:
        """Check cache for existing analysis results."""
        cached_results = []
        
        for file_change in request.file_changes:
            cache_key = self._generate_cache_key(file_change)
            
            if cache_key in self._result_cache:
                cached_result = self._result_cache[cache_key]
                # Update access time
                self._cache_access_times[cache_key] = time.time()
                cached_result.cache_hit = True
                cached_results.append(cached_result)
                self._stats["cache_hits"] += 1
            else:
                self._stats["cache_misses"] += 1
        
        return cached_results if len(cached_results) == len(request.file_changes) else []
    
    def _generate_cache_key(self, file_change: FileChange) -> str:
        """Generate cache key for file change."""
        key_data = f"{file_change.file_path}:{file_change.content_hash}:{file_change.change_type}"
        return hashlib.sha256(key_data.encode()).hexdigest()[:16]
    
    async def _analyze_file_change(self, 
                                    file_change: FileChange, 
                                    request: AnalysisRequest) -> Optional[AnalysisResult]:
        """Analyze individual file change."""
        try:
            # Create analyzer instance
            analyzer = self.analyzer_factory()
            
            # Determine analysis type based on change
            if file_change.change_type == 'created':
                # Full analysis for new files
                violations = await self._run_full_analysis(analyzer, file_change.file_path)
            else:
                # Incremental analysis for modifications
                violations = await self._run_incremental_analysis(analyzer, file_change)
            
            return AnalysisResult(
                request_id=request.request_id,
                file_path=str(file_change.file_path),
                violations=violations,
                processing_time_ms=0,  # Updated in _process_request
                analysis_type=request.analysis_type,
                timestamp=time.time(),
                cache_hit=False
            )
            
        except Exception as e:
            logger.error(f"Analysis failed for {file_change.file_path}: {e}")
            return None
    
    async def _run_full_analysis(self, analyzer: Any, file_path: Path) -> List[Dict[str, Any]]:
        """Run full analysis on file using existing analyzer."""
        try:
            # Check if analyzer has the analyze_file method
            if hasattr(analyzer, 'analyze_file'):
                result = analyzer.analyze_file(str(file_path))
                if hasattr(result, 'violations'):
                    return [self._violation_to_dict(v) for v in result.violations]
                elif isinstance(result, dict) and 'violations' in result:
                    return result['violations']
            
            # Fallback: try to run basic AST analysis
            if hasattr(analyzer, 'ast_analyzer') and analyzer.ast_analyzer:
                with open(file_path, 'r', encoding='utf-8') as f:
                    source_code = f.read()
                    source_lines = source_code.splitlines()
                
                import ast
                tree = ast.parse(source_code)
                
                # Use existing AST analyzer
                violations = analyzer.ast_analyzer.analyze_file(str(file_path), tree, source_lines)
                return [self._violation_to_dict(v) for v in violations]
            
        except Exception as e:
            logger.error(f"Full analysis failed for {file_path}: {e}")
        
        return []
    
    async def _run_incremental_analysis(self, 
                                        analyzer: Any, 
                                        file_change: FileChange) -> List[Dict[str, Any]]:
        """Run incremental analysis on file change using delta optimization."""
        try:
            # Import incremental cache for delta tracking
            from .incremental_cache import get_global_incremental_cache
            
            incremental_cache = get_global_incremental_cache()
            file_path = file_change.file_path
            
            # Check cache for existing results
            current_hash = file_change.content_hash
            cached_result = incremental_cache.get_partial_result(
                file_path, "violations", current_hash
            )
            
            if cached_result:
                logger.debug(f"Using cached incremental result for {file_path}")
                return cached_result.data if isinstance(cached_result.data, list) else []
            
            # Track the file change for delta processing
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    new_content = f.read()
            except Exception as e:
                logger.warning(f"Could not read {file_path}: {e}")
                new_content = ""
            
            delta = incremental_cache.track_file_change(file_path, None, new_content)
            
            # For now, run full analysis (incremental AST diff would be implemented here)
            violations = await self._run_full_analysis(analyzer, file_path)
            
            # Cache the results
            if violations and current_hash:
                incremental_cache.store_partial_result(
                    file_path, "violations", violations, current_hash,
                    dependencies=set(),  # Would extract imports/dependencies
                    metadata={"delta_analysis": True, "change_type": file_change.change_type}
                )
            
            return violations
            
        except Exception as e:
            logger.error(f"Incremental analysis failed for {file_change.file_path}: {e}")
            # Fallback to full analysis
            return await self._run_full_analysis(analyzer, file_change.file_path)
    
    def _violation_to_dict(self, violation: Any) -> Dict[str, Any]:
        """Convert violation object to dictionary format."""
        if isinstance(violation, dict):
            return violation
        
        # Handle different violation object types
        if hasattr(violation, '__dict__'):
            return violation.__dict__
        elif hasattr(violation, '_asdict'):
            return violation._asdict()
        else:
            return {"description": str(violation), "type": "unknown"}
    
    def _handle_file_deletion(self, 
                            file_change: FileChange, 
                            request: AnalysisRequest) -> AnalysisResult:
        """Handle file deletion by clearing related violations."""
        return AnalysisResult(
            request_id=request.request_id,
            file_path=str(file_change.file_path),
            violations=[],  # No violations for deleted file
            processing_time_ms=0,
            analysis_type="deletion",
            timestamp=time.time(),
            metadata={"deleted": True}
        )
    
    def _cache_results(self, request: AnalysisRequest, results: List[AnalysisResult]) -> None:
        """Cache analysis results with LRU eviction."""
        current_time = time.time()
        
        for i, file_change in enumerate(request.file_changes):
            if i < len(results):
                cache_key = self._generate_cache_key(file_change)
                self._result_cache[cache_key] = results[i]
                self._cache_access_times[cache_key] = current_time
        
        # Enforce cache size limit (NASA Rule 7)
        if len(self._result_cache) > self.cache_size:
            self._evict_old_cache_entries()
    
    def _evict_old_cache_entries(self) -> None:
        """Evict oldest cache entries to maintain size limit."""
        # Sort by access time and remove oldest
        sorted_entries = sorted(
            self._cache_access_times.items(),
            key=lambda x: x[1]
        )
        
        # Remove oldest 20% of entries
        evict_count = len(sorted_entries) // 5
        for cache_key, _ in sorted_entries[:evict_count]:
            self._result_cache.pop(cache_key, None)
            self._cache_access_times.pop(cache_key, None)
    
    async def _emit_results(self, results: List[AnalysisResult]) -> None:
        """Emit analysis results to callbacks and queues."""
        for result in results:
            # Add to results queue
            try:
                await self._results_queue.put(result)
            except asyncio.QueueFull:
                logger.warning("Results queue full - dropping result")
            
            # Call individual result callbacks
            for callback in self._result_callbacks:
                try:
                    callback(result)
                except Exception as e:
                    logger.error(f"Result callback failed: {e}")
        
        # Call batch callbacks
        if results:
            for callback in self._batch_callbacks:
                try:
                    callback(results)
                except Exception as e:
                    logger.error(f"Batch callback failed: {e}")
    
    def add_result_callback(self, callback: Callable[[AnalysisResult], None]) -> None:
        """Add callback for individual analysis results."""
        assert callable(callback), "callback must be callable"
        self._result_callbacks.append(callback)
    
    def add_batch_callback(self, callback: Callable[[List[AnalysisResult]], None]) -> None:
        """Add callback for batched analysis results."""
        assert callable(callback), "callback must be callable"
        self._batch_callbacks.append(callback)
    
    async def get_results(self, timeout: Optional[float] = None) -> AsyncGenerator[AnalysisResult, None]:
        """Get analysis results as async generator."""
        while self._running or not self._results_queue.empty():
            try:
                result = await asyncio.wait_for(
                    self._results_queue.get(), 
                    timeout=timeout or 1.0
                )
                yield result
            except asyncio.TimeoutError:
                if not self._running:
                    break
    
    def get_stats(self) -> Dict[str, Any]:
        """Get processing statistics."""
        total_requests = self._stats["requests_processed"]
        avg_processing_time = (
            self._stats["processing_time_ms"] / total_requests 
            if total_requests > 0 else 0
        )
        
        cache_requests = self._stats["cache_hits"] + self._stats["cache_misses"]
        cache_hit_rate = (
            self._stats["cache_hits"] / cache_requests 
            if cache_requests > 0 else 0
        )
        
        return {
            "requests_processed": total_requests,
            "average_processing_time_ms": avg_processing_time,
            "cache_hit_rate": cache_hit_rate,
            "cache_size": len(self._result_cache),
            "queue_size": self._processing_queue.qsize(),
            "results_pending": self._results_queue.qsize(),
            "queue_overflows": self._stats["queue_overflows"],
            "dependency_invalidations": self._stats["dependency_invalidations"]
        }
    
    async def __aenter__(self):
        """Async context manager entry."""
        await self.start()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        await self.stop()

# Factory functions and utilities

def create_stream_processor(analyzer_factory: Callable[[], Any], **kwargs) -> StreamProcessor:
    """Factory function to create configured stream processor."""
    return StreamProcessor(analyzer_factory=analyzer_factory, **kwargs)

async def process_file_changes_stream(
    directories: List[Union[str, Path]],
    analyzer_factory: Optional[Callable[[], Any]] = None,
    result_callback: Optional[Callable[[AnalysisResult], None]] = None
) -> AsyncGenerator[AnalysisResult, None]:
    """
    High-level function for streaming file change analysis.

    Args:
        directories: Directories to watch for changes
        analyzer_factory: Factory function for analyzer instances
        result_callback: Optional callback for results

    Yields:
        Analysis results as they become available
    """
    processor = StreamProcessor(analyzer_factory)

    if result_callback:
        processor.add_result_callback(result_callback)

    async with processor:
        # Start watching directories
        processor.start_watching(directories)

        # Yield results as they arrive
        async for result in processor.get_results():
            yield result